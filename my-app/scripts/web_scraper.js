class WebScraper {
  constructor() {
    this.userAgent = "HLounh-Browser/1.0.0"
  }

  async fetchPage(url) {
    try {
      console.log(`ƒêang t·∫£i trang: ${url}`)

      const response = await fetch(url, {
        headers: {
          "User-Agent": this.userAgent,
        },
      })

      if (!response.ok) {
        throw new Error(`HTTP ${response.status}: ${response.statusText}`)
      }

      const html = await response.text()
      return {
        success: true,
        url: url,
        status: response.status,
        html: html,
        size: html.length,
      }
    } catch (error) {
      return {
        success: false,
        url: url,
        error: error.message,
      }
    }
  }

  extractMetadata(html) {
    const metadata = {
      title: "",
      description: "",
      keywords: "",
      author: "",
      favicon: "",
      images: [],
      links: [],
    }

    try {
      // Extract title
      const titleMatch = html.match(/<title[^>]*>([^<]+)<\/title>/i)
      if (titleMatch) {
        metadata.title = titleMatch[1].trim()
      }

      // Extract meta description
      const descMatch = html.match(/<meta[^>]*name=["']description["'][^>]*content=["']([^"']+)["']/i)
      if (descMatch) {
        metadata.description = descMatch[1].trim()
      }

      // Extract meta keywords
      const keywordsMatch = html.match(/<meta[^>]*name=["']keywords["'][^>]*content=["']([^"']+)["']/i)
      if (keywordsMatch) {
        metadata.keywords = keywordsMatch[1].trim()
      }

      // Extract favicon
      const faviconMatch = html.match(/<link[^>]*rel=["'](?:shortcut )?icon["'][^>]*href=["']([^"']+)["']/i)
      if (faviconMatch) {
        metadata.favicon = faviconMatch[1]
      }

      // Extract images
      const imgMatches = html.matchAll(/<img[^>]*src=["']([^"']+)["'][^>]*>/gi)
      for (const match of imgMatches) {
        metadata.images.push(match[1])
      }

      // Extract links
      const linkMatches = html.matchAll(/<a[^>]*href=["']([^"']+)["'][^>]*>([^<]*)<\/a>/gi)
      for (const match of linkMatches) {
        metadata.links.push({
          url: match[1],
          text: match[2].trim(),
        })
      }
    } catch (error) {
      console.error("L·ªói khi ph√¢n t√≠ch metadata:", error)
    }

    return metadata
  }

  async analyzeWebsite(url) {
    console.log("=== PH√ÇN T√çCH WEBSITE ===")
    console.log(`URL: ${url}`)

    const result = await this.fetchPage(url)

    if (!result.success) {
      console.log(`‚ùå L·ªói: ${result.error}`)
      return result
    }

    console.log(`‚úÖ T·∫£i th√†nh c√¥ng (${result.size} bytes)`)

    const metadata = this.extractMetadata(result.html)

    console.log("\nüìã TH√îNG TIN TRANG:")
    console.log(`Ti√™u ƒë·ªÅ: ${metadata.title || "Kh√¥ng c√≥"}`)
    console.log(`M√¥ t·∫£: ${metadata.description || "Kh√¥ng c√≥"}`)
    console.log(`T·ª´ kh√≥a: ${metadata.keywords || "Kh√¥ng c√≥"}`)
    console.log(`Favicon: ${metadata.favicon || "Kh√¥ng c√≥"}`)

    console.log(`\nüñºÔ∏è H√åNH ·∫¢NH: ${metadata.images.length} ·∫£nh`)
    metadata.images.slice(0, 5).forEach((img, index) => {
      console.log(`  ${index + 1}. ${img}`)
    })
    if (metadata.images.length > 5) {
      console.log(`  ... v√† ${metadata.images.length - 5} ·∫£nh kh√°c`)
    }

    console.log(`\nüîó LI√äN K·∫æT: ${metadata.links.length} link`)
    metadata.links.slice(0, 10).forEach((link, index) => {
      console.log(`  ${index + 1}. ${link.text} -> ${link.url}`)
    })
    if (metadata.links.length > 10) {
      console.log(`  ... v√† ${metadata.links.length - 10} link kh√°c`)
    }

    return {
      ...result,
      metadata: metadata,
    }
  }

  async checkMultipleUrls(urls) {
    console.log("=== KI·ªÇM TRA NHI·ªÄU WEBSITE ===")

    const results = []

    for (const url of urls) {
      console.log(`\n${"=".repeat(50)}`)
      const result = await this.analyzeWebsite(url)
      results.push(result)

      // Delay ƒë·ªÉ tr√°nh spam
      await new Promise((resolve) => setTimeout(resolve, 1000))
    }

    console.log("\nüìä T·ªîNG K·∫æT:")
    const successful = results.filter((r) => r.success).length
    console.log(`‚úÖ Th√†nh c√¥ng: ${successful}/${urls.length}`)
    console.log(`‚ùå Th·∫•t b·∫°i: ${urls.length - successful}/${urls.length}`)

    return results
  }
}

// S·ª≠ d·ª•ng Web Scraper
const scraper = new WebScraper()

// Test v·ªõi m·ªôt s·ªë website ph·ªï bi·∫øn
const testUrls = [
  "https://www.google.com",
  "https://github.com",
  "https://stackoverflow.com",
  "https://developer.mozilla.org",
]

console.log("üåê HLounh Browser Web Scraper")
console.log("C√¥ng c·ª• ph√¢n t√≠ch v√† thu th·∫≠p th√¥ng tin website\n")

// Ch·∫°y test
scraper
  .checkMultipleUrls(testUrls)
  .then((results) => {
    console.log("\nüéâ Ho√†n th√†nh ph√¢n t√≠ch!")
  })
  .catch((error) => {
    console.error("‚ùå L·ªói:", error)
  })

export default WebScraper
